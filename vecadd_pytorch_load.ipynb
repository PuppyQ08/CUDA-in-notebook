{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOfk9soS3T/aXvPb31Nxgce",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PuppyQ08/CUDA-in-notebook/blob/main/vecadd_pytorch_load.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hi, I am QQY. Here I used the pytorch load inline to customize the vecadd kernel and do profiling. Let's how it works!"
      ],
      "metadata": {
        "id": "yO27MDeoF0Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lnc6hKmJc3c",
        "outputId": "0188f2fa-9f92-41e0-b436-86cfe6004c81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/180.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8dfrXyTHFvIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaa468d1-c52d-44b1-bcb3-f5b39abd80cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.,  4.,  9.],\n",
            "        [16., 25., 36.]], device='cuda:0')\n",
            "Execution time of square_matrix_ext.square_matrix: 0.356 ms\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "\n",
        "# Define the CUDA kernel and C++ wrapper\n",
        "cuda_source = '''\n",
        "__global__ void square_matrix_kernel(const float* matrix, float* result, int width, int height) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < height && col < width) {\n",
        "        int idx = row * width + col;\n",
        "        result[idx] = matrix[idx] * matrix[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "torch::Tensor square_matrix(torch::Tensor matrix) {\n",
        "    const auto height = matrix.size(0);\n",
        "    const auto width = matrix.size(1);\n",
        "\n",
        "    auto result = torch::empty_like(matrix);\n",
        "\n",
        "    dim3 threads_per_block(16, 16);\n",
        "    dim3 number_of_blocks((width + threads_per_block.x - 1) / threads_per_block.x,\n",
        "                          (height + threads_per_block.y - 1) / threads_per_block.y);\n",
        "\n",
        "    square_matrix_kernel<<<number_of_blocks, threads_per_block>>>(\n",
        "        matrix.data_ptr<float>(), result.data_ptr<float>(), width, height);\n",
        "\n",
        "    return result;\n",
        "    }\n",
        "'''\n",
        "\n",
        "cpp_source = \"torch::Tensor square_matrix(torch::Tensor matrix);\"\n",
        "\n",
        "# Load the CUDA kernel as a PyTorch extension\n",
        "square_matrix_ext = load_inline(\n",
        "    name='square_matrix_ext',\n",
        "    cpp_sources=cpp_source,\n",
        "    cuda_sources=cuda_source,\n",
        "    functions=['square_matrix'],\n",
        "    with_cuda=True,\n",
        "    extra_cuda_cflags=[\"-O2\", \"-gencode=arch=compute_75,code=sm_75\"],\n",
        "    build_directory='./',\n",
        "    # extra_cuda_cflags=['--expt-relaxed-constexpr']\n",
        ")\n",
        "\n",
        "a = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device='cuda')\n",
        "\n",
        "# Profiling the execution time\n",
        "start_event = torch.cuda.Event(enable_timing=True)\n",
        "end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "start_event.record()\n",
        "result = square_matrix_ext.square_matrix(a)\n",
        "end_event.record()\n",
        "torch.cuda.synchronize() # Wait for the events to complete\n",
        "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
        "\n",
        "print(result)\n",
        "print(f\"Execution time of square_matrix_ext.square_matrix: {elapsed_time_ms:.3f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d699aa86",
        "outputId": "771a0c7d-b79a-4adb-9018-74e90b1140f6"
      },
      "source": [
        "import torch\n",
        "from torch.utils.cpp_extension import load\n",
        "import os\n",
        "\n",
        "# Define the CUDA kernel source code\n",
        "cuda_source_content = '''\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void square_matrix_kernel(const float* matrix, float* result, int width, int height) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < height && col < width) {\n",
        "        int idx = row * width + col;\n",
        "        result[idx] = matrix[idx] * matrix[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "// C++ wrapper for the CUDA kernel\n",
        "torch::Tensor square_matrix_cuda(torch::Tensor matrix) {\n",
        "    const auto height = matrix.size(0);\n",
        "    const auto width = matrix.size(1);\n",
        "\n",
        "    auto result = torch::empty_like(matrix);\n",
        "\n",
        "    dim3 threads_per_block(16, 16);\n",
        "    dim3 number_of_blocks((width + threads_per_block.x - 1) / threads_per_block.x,\n",
        "                          (height + threads_per_block.y - 1) / threads_per_block.y);\n",
        "\n",
        "    square_matrix_kernel<<<number_of_blocks, threads_per_block>>>(\n",
        "        matrix.data_ptr<float>(), result.data_ptr<float>(), width, height);\n",
        "\n",
        "    return result;\n",
        "}\n",
        "'''\n",
        "\n",
        "# Define the C++ source code with Pybind11 bindings\n",
        "cpp_source_content = '''\n",
        "#include <torch/extension.h>\n",
        "\n",
        "torch::Tensor square_matrix_cuda(torch::Tensor matrix);\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"square_matrix\", &square_matrix_cuda, \"Square matrix CUDA kernel\");\n",
        "}\n",
        "'''\n",
        "\n",
        "# Write the source codes to files\n",
        "with open('square_matrix_cuda.cu', 'w') as f:\n",
        "    f.write(cuda_source_content)\n",
        "\n",
        "with open('square_matrix_cpp.cpp', 'w') as f:\n",
        "    f.write(cpp_source_content)\n",
        "\n",
        "# Load the CUDA kernel as a PyTorch extension\n",
        "square_matrix_ext_load = load(\n",
        "    name='square_matrix_ext_load',\n",
        "    sources=['square_matrix_cpp.cpp', 'square_matrix_cuda.cu'],\n",
        "    extra_cuda_cflags=[\"-O2\", \"-gencode=arch=compute_75,code=sm_75\"],\n",
        "    is_python_module=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "a = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device='cuda')\n",
        "\n",
        "# Profiling the execution time\n",
        "start_event = torch.cuda.Event(enable_timing=True)\n",
        "end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "start_event.record()\n",
        "result_load = square_matrix_ext_load.square_matrix(a)\n",
        "end_event.record()\n",
        "torch.cuda.synchronize() # Wait for the events to complete\n",
        "elapsed_time_ms_load = start_event.elapsed_time(end_event)\n",
        "\n",
        "print(\"Result using load style:\")\n",
        "print(result_load)\n",
        "print(f\"Execution time of square_matrix_ext_load.square_matrix: {elapsed_time_ms_load:.3f} ms\")\n",
        "\n",
        "# Clean up the generated files (optional)\n",
        "# os.remove('square_matrix_cuda.cu')\n",
        "# os.remove('square_matrix_cpp.cpp')\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result using load style:\n",
            "tensor([[ 1.,  4.,  9.],\n",
            "        [16., 25., 36.]], device='cuda:0')\n",
            "Execution time of square_matrix_ext_load.square_matrix: 0.604 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jesus, it is super slow....It talke 1 min to compile. It lost the point for quick showing."
      ],
      "metadata": {
        "id": "uGQRenQ1Phu3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zsiq-M3YJLL7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}